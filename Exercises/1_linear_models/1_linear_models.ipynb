{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1: Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "In ordinary least squares regression, we fit a linear relationship to the data such that the square errors are the smallest. That is, we solve for the coefficients such that\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{OLS} = \\underset{\\beta}{\\mathrm{argmin}} {||y-X\\beta||_2^2} = \\underset{\\beta}{\\mathrm{argmin}} (y - {X}\\beta)^T(y - {X}\\beta)\n",
    "$$\n",
    "\n",
    "The goal of this exercise is to create our own linear regression model in numpy, and then compare it to a built-in implementation in the scikit-learn package. Define your linear regression model by completing the class `MyLinearModel`, fit it to the generated data and print out the coefficients. Does the scikit model produce the same coefficients? Similarly, complete the function `my_r2_score()` to evaluate the $R^2$ of your model on the generated data. Check your answer with the scikit function `r2_score`.\n",
    "\n",
    "Note: When fitting scikit models in this exercise, set `fit_intercept=False` since the intercept is accounted for in the data matrix by a column of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3) (6, 1)\n"
     ]
    }
   ],
   "source": [
    "# True coefficients\n",
    "true_beta = np.array([10, 5, 5])\n",
    "# Data matrix (first column is 1 for the intercept)\n",
    "X = np.array([[1, 7, 0],\n",
    "             [1, 15, 0],\n",
    "             [1, 3, 0],\n",
    "             [1, 0, 0.31],\n",
    "             [1, 0, 0.35],\n",
    "             [1, 0, 0.33]])\n",
    "# Generate Y as linear function of X with some Gaussian noise\n",
    "y = X @ true_beta.reshape(-1, 1) + np.random.normal(size=(6, 1))\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"Implement a linear regression model\"\"\"\n",
    "        # Save the fitted coefficients\n",
    "        self.coefficients = None\n",
    "        # Track if the model has been fit yet\n",
    "        self.isfit = False\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        X: array of shape (n, p)\n",
    "        y: array of shape (n, 1)\n",
    "        \"\"\"\n",
    "        # Compute and save the fitted coefficients\n",
    "        self.isfit = True\n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        X: array of shape (n, p)\n",
    "        \"\"\"\n",
    "        if self.isfit == False:\n",
    "            raise Exception('Model must be fitted first')\n",
    "        \n",
    "        # Return the predicted values\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_r2_score(y_true, y_pred):\n",
    "    \"\"\" \n",
    "    Evaluate the r2 score on the data\n",
    "    Params:\n",
    "    y_true - array of shape (n, 1), the true y labels\n",
    "    y_pred - array of shape (n, 1), the predicted y labels\n",
    "    Returns:\n",
    "    r2 - float, the coefficient of determination\n",
    "    \"\"\"\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge Regression\n",
    "Ridge regularizes the coefficients using the L2-norm, trading in some bias to reduce the variance of the model. That is, instead of the least square, we solve\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{ridge} = \\underset{\\beta}{\\mathrm{argmin}} {||y-X\\beta||_2^2 + \\alpha||\\beta||_2^2} = \\underset{\\beta}{\\mathrm{argmin}} (y - {X}\\beta)^T(y - {X}\\beta) + \\alpha\\beta^T\\beta\n",
    "$$\n",
    "\n",
    "a) As before, create your own ridge regression model in numpy, and then compare it to a built-in implementation in the scikit-learn package. Define your model as the class `MyRidgeModel`, and fit it to the generated data. Print out the coefficients after setting the shrinkage parameter `alpha=0`, and compare them to the linear regression coefficients you got for Exercise 1. What do you notice? Then, fit the model with `alpha=0.1`, and check your coefficients compared to the equivalent scikit model. \n",
    "\n",
    "b) Next, plot how the coefficients $\\beta_1$ and $\\beta_2$ change with $\\alpha=0, 5, 10, 15, 20, 25$ (ignoring the intercept). Which coefficient is shrunk the most? Explain why this is and what this tells us about ridge regression. \n",
    "\n",
    "\n",
    "Hint: plotting a scatterplot of the data will help. Since there is no covariance between $x_1$ and $x_2$, the \"principal directions\" are the $x_1$ and $x_2$ directions themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRidgeModel:\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\"\n",
    "        Implement a ridge regression model\n",
    "        Parmas:\n",
    "        alpha - float, regulaization factor\n",
    "        \"\"\"\n",
    "        self.coefficients = None\n",
    "        self.alpha = alpha\n",
    "        self.isfit = False\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        X: array of shape (n, p)\n",
    "        y: array of shape (n, 1)\n",
    "        \"\"\"\n",
    "        # Compute and save the fitted coefficients\n",
    "        self.isfit = True\n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        X: array of shape (n, p)\n",
    "        \"\"\"\n",
    "        if self.isfit == False:\n",
    "            raise Exception('Model must be fitted first')\n",
    "        \n",
    "        # Return the predicted values\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LASSO Regression\n",
    "Fit a LASSO model to the data (the scikit implementation is ok), and create a plot comparing the penalty factor $\\alpha$ and the coefficients $\\beta_1$ and $\\beta_2$ as before. Do this for $\\alpha=1, 5, 10, 15, 20, 25$. How does LASSO shrink the coefficients compared to ridge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. California Housing Dataset\n",
    "Let's practice linear regression using a real dataset. The California Housing Dataset reports the median house price of different blocks in California, as well as a variety of other features such as median income, house age, # bedrooms, # bathrooms, etc. Use 5-fold cross-validation on the training data to compare different linear regression, ridge, and lasso models, and play with feature selection. Test the best model on the hold-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and targets (y)\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
